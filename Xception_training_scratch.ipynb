{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CaOFa5vCNziq",
    "outputId": "66519c02-31d7-4902-8f33-3ff44e027bab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fea08c983d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "#%matplotlib inline\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "manualSeed = 500 #999\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fflooPh6OKOh"
   },
   "outputs": [],
   "source": [
    "dataroot = \"/celeba\"\n",
    "\n",
    "# Number of workers for dataloader\n",
    "workers = 2\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 256 #128\n",
    "\n",
    "# Spatial size of training images. All images will be resized to this\n",
    "#   size using a transformer.\n",
    "image_size = 64\n",
    "\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 3\n",
    "\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = 100\n",
    "\n",
    "# Size of feature maps in generator\n",
    "ngf = 64\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "ndf = 64\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 2\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr = 0.000002 #0.0002\n",
    "\n",
    "# Beta1 hyperparam for Adam optimizers\n",
    "beta1 = 0.5\n",
    "\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = 1\n",
    "\n",
    "pretrained_generator_model_path = '/content/drive/MyDrive/CV_Proj/DCGAN_generator.pth'\n",
    "pretrained_discriminator_model_path = '/content/drive/My Drive/DCGAN_discriminator_model_6.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "xExD1JxaOga6"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "AP7rbQE_P4zH"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "from cnn_finetune import make_model\n",
    "\n",
    "def make_classifier(in_features, num_classes):\n",
    "  return nn.Sequential(nn.Linear(in_features,num_classes), nn.Sigmoid())\n",
    "\n",
    "\n",
    "\n",
    "disc_3 = make_model('xception', num_classes=1, pretrained=False, input_size=(64,64), classifier_factory = make_classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "lR92qdSWRKlJ"
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "\n",
    "        self.model = disc_3.to(device)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return self.model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VH23hQU8Rf8s",
    "outputId": "3754dcda-bd66-40d9-f033-e2e65bf97253"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "torch.Size([3200, 3, 64, 64]) torch.Size([3200])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "momentum = 0.9\n",
    "# lr = 0.001\n",
    "epochs = 10\n",
    "log_interval = 10\n",
    "\n",
    "count1 = 0\n",
    "count2 = 0\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# Number of workers for dataloader\n",
    "workers = 2\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 128\n",
    "\n",
    "# Spatial size of training images. All images will be resized to this\n",
    "#   size using a transformer.\n",
    "image_size = 64\n",
    "\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 3\n",
    "\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = 100\n",
    "\n",
    "# Size of feature maps in generator\n",
    "ngf = 64\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "ndf = 64\n",
    "\n",
    "ngpu = 0\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n",
    "model_G = Generator(ngpu)#.to(device)\n",
    "model_G.load_state_dict(torch.load('/content/drive/MyDrive/CV_Proj/DCGAN_generator.pth', map_location=torch.device('cpu'))) #.to(device)\n",
    "\n",
    "for i in range(50):\n",
    "  count1 += 64 \n",
    "  fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "  with torch.no_grad():\n",
    "    fake = model_G(fixed_noise).detach().cpu()\n",
    "  X.append(fake)\n",
    "  y.append(torch.zeros(len(fake)))\n",
    "\n",
    "X =torch.cat(X, 0)\n",
    "y = torch.cat(y,0)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U_K3K1WsRkRu",
    "outputId": "4c692494-6213-4830-a8ec-bcaf8cb3cb0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200 3200\n"
     ]
    }
   ],
   "source": [
    "dataroot = \"/celeba\"\n",
    "import glob\n",
    "count2 = 0\n",
    "import numpy as np \n",
    "from PIL import Image\n",
    "\n",
    "transform=transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.CenterCrop(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "for i in glob.glob(dataroot + '/img_align_celeba/img_align_celeba/*'):\n",
    "  count2+=1 \n",
    "  if (count2<=3200):\n",
    "    im = Image.open(i)\n",
    "    im = transform(im)\n",
    "    im = im.unsqueeze(0)\n",
    "    \n",
    "    X = torch.cat((X,im ), 0)\n",
    "    y = torch.cat((y, torch.tensor(1).unsqueeze(0)), 0)\n",
    "  if(count2>=3200):\n",
    "    break \n",
    "\n",
    "print(count1, count2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZczyTBl5Teyt",
    "outputId": "6043ddd0-8afa-436b-cfb6-4208a8438107"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import cv2 \n",
    "import glob\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = torch.tensor(X_train), torch.tensor(X_test), torch.tensor(y_train), torch.tensor(y_test) \n",
    "\n",
    "\n",
    "X_train.shape\n",
    "\n",
    "\n",
    "my_dataset_train = TensorDataset(X_train, y_train) # create your datset\n",
    "train_loader = DataLoader(my_dataset_train, batch_size=32, shuffle=True)\n",
    "\n",
    "my_dataset_test = TensorDataset(X_test, y_test) # create your datset\n",
    "test_loader = DataLoader(my_dataset_test, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "R_tkQPfmUfc5"
   },
   "outputs": [],
   "source": [
    "model = Discriminator(ngpu=1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WGZJujg6UuiB",
    "outputId": "846f431a-91a3-4ebb-e08c-46f986e4e481"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [5408/5440 (99%)]\tLoss: 0.202747\tAccuracy88.346\n",
      "\n",
      "Validation set: Average loss: 0.0005, Accuracy: 782/960 (81%)\n",
      "\n",
      "\n",
      "Saved model to DCGAN_discriminator_model_1.pth.\n",
      "Train Epoch: 2 [5408/5440 (99%)]\tLoss: 0.019160\tAccuracy98.290\n",
      "\n",
      "Validation set: Average loss: 0.0000, Accuracy: 950/960 (99%)\n",
      "\n",
      "\n",
      "Saved model to DCGAN_discriminator_model_2.pth.\n",
      "Train Epoch: 3 [5408/5440 (99%)]\tLoss: 0.000436\tAccuracy98.971\n",
      "\n",
      "Validation set: Average loss: 0.0000, Accuracy: 949/960 (99%)\n",
      "\n",
      "\n",
      "Saved model to DCGAN_discriminator_model_3.pth.\n",
      "Train Epoch: 4 [5408/5440 (99%)]\tLoss: 0.149118\tAccuracy99.375\n",
      "\n",
      "Validation set: Average loss: 0.0000, Accuracy: 908/960 (95%)\n",
      "\n",
      "\n",
      "Saved model to DCGAN_discriminator_model_4.pth.\n",
      "Train Epoch: 5 [5408/5440 (99%)]\tLoss: 0.001623\tAccuracy99.081\n",
      "\n",
      "Validation set: Average loss: 0.0000, Accuracy: 953/960 (99%)\n",
      "\n",
      "\n",
      "Saved model to DCGAN_discriminator_model_5.pth.\n",
      "Train Epoch: 6 [5408/5440 (99%)]\tLoss: 0.000150\tAccuracy99.890\n",
      "\n",
      "Validation set: Average loss: 0.0000, Accuracy: 957/960 (100%)\n",
      "\n",
      "\n",
      "Saved model to DCGAN_discriminator_model_6.pth.\n",
      "Train Epoch: 7 [5408/5440 (99%)]\tLoss: 0.002358\tAccuracy99.577\n",
      "\n",
      "Validation set: Average loss: 0.0000, Accuracy: 953/960 (99%)\n",
      "\n",
      "\n",
      "Saved model to DCGAN_discriminator_model_7.pth.\n",
      "Train Epoch: 8 [5408/5440 (99%)]\tLoss: 0.000007\tAccuracy99.743\n",
      "\n",
      "Validation set: Average loss: 0.0000, Accuracy: 958/960 (100%)\n",
      "\n",
      "\n",
      "Saved model to DCGAN_discriminator_model_8.pth.\n",
      "Train Epoch: 9 [5408/5440 (99%)]\tLoss: 0.000006\tAccuracy99.871\n",
      "\n",
      "Validation set: Average loss: 0.0000, Accuracy: 948/960 (99%)\n",
      "\n",
      "\n",
      "Saved model to DCGAN_discriminator_model_9.pth.\n",
      "Train Epoch: 10 [5408/5440 (99%)]\tLoss: 0.000037\tAccuracy99.669\n",
      "\n",
      "Validation set: Average loss: 0.0000, Accuracy: 956/960 (100%)\n",
      "\n",
      "\n",
      "Saved model to DCGAN_discriminator_model_10.pth.\n"
     ]
    }
   ],
   "source": [
    "lr = 0.0005\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "batch_size = 64\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "loss_func = nn.BCELoss()\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    correct =0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device).float(), target.to(device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data).view(-1).to(device)\n",
    "        loss = loss_func(output, target.to(device).float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pred = torch.zeros(output.data.shape).to(device)\n",
    "        pred[output.data >=0.5] = 1\n",
    "\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum().to(device)\n",
    "    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tAccuracy{:.3f}'.format(\n",
    "        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "        100. * batch_idx / len(train_loader), loss.item(), 100. * correct / len(train_loader.dataset) ))\n",
    "\n",
    "def validation():\n",
    "    model.eval()\n",
    "    validation_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data = data.to(device).float()\n",
    "        target = target.to(device).long()\n",
    "        output = model(data).view(-1)\n",
    "        validation_loss = loss_func(output, target.float())\n",
    "        pred = torch.zeros(output.data.shape).to(device)\n",
    "        pred[output.data >=0.5] = 1\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    validation_loss /= len(test_loader.dataset)\n",
    "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        validation_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    validation()\n",
    "    model_file = 'DCGAN_discriminator_model_' + str(epoch) + '.pth'\n",
    "    torch.save(model, model_file)\n",
    "    print('\\nSaved model to ' + model_file + '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KiXUFv8jAqgX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Exception_scratch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
